# -*- coding: utf-8 -*-
"""ВКРБ.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RuGLxALnixYwnGgJ-b7n2zTXx5UuAzdQ

# Система анализа алгоритмов машинного обучения для решения задач классификации с использованием Pandas

## Введение
В качестве предметной области был выбран набор данных, содержащий информацию о численности экономически активного населения, безработных, уровне безработницы и сопоставляющий эти покказатели между различными возрастными группами по субъектам РФ.

### Выбор набора данных для построения моделей машинного обучения
Данный набор данных доступен по адресу: https://data.rcsi.science/data-catalog/datasets/156/#dataset-overview

Набор данных имеет следующие атрибуты:

* territory - Наименование территории по ОКАТО
* num_economactivepopulation_all - 	Численность экономически активного населения
* employed_num_all - Занятые в экономике
* unemployed_num_all - Безработные
* eactivity_lvl - Уровень экономической активности
* employment_lvl - Уровень занятости
* unemployment_lvl - Уровень безработицы
* dis_unagegroup_to20 - Распределение безработных в возрасте до 20 лет по регионам РФ
* dis_unagegroup_20-29 - Распределение безработных в возрасте от 20 до 29 лет по регионам РФ
* dis_unagegroup_30-39 - Распределение безработных в возрасте от 30 до 39 лет по регионам РФ
* dis_unagegroup_40-49 - Распределение безработных в возрасте от 40 до 49 лет по регионам РФ
* dis_unagegroup_50-59 - Распределение безработных в возрасте от 50 до 59 лет по регионам РФ
* dis_unagegroup_60older - Распределение безработных в возрасте 60 и более лет по регионам РФ
* dis_emagegroup_to20 - Распределение занятых в экономике в возрасте до 20 лет по регионам РФ
* dis_emagegroup_20-29 - Распределение занятых в экономике в возрасте от 20 до 29 лет по регионам РФ
* dis_emagegroup_30-39 - Распределение занятых в экономике в возрасте от 30 до 39 лет по регионам РФ
* dis_emagegroup_40-49 - Распределение занятых в экономике в возрасте от 40 до 49 лет по регионам РФ
* dis_emagegroup_50-59 - Распределение занятых в экономике в возрасте от 50 до 59 лет по регионам РФ
* dis_emagegroup_60older - Распределение занятых в экономике в возрасте 60 и более лет по регионам РФ
* num_unagegroup_to20 - Численность безработных в возрасте до 20 лет по регионам РФ
* num_unagegroup_20-29 - Численность безработных в возрасте от 20 до 29 лет по регионам РФ
* num_unagegroup_30-39 - Численность безработных в возрасте от 30 до 39 лет по регионам РФ
* num_unagegroup_40-49 - Численность безработных в возрасте от 40 до 49 лет по регионам РФ
* num_unagegroup_50-59 - Численность безработных в возрасте от 50 до 59 лет по регионам РФ
* num_unagegroup_60older - Численность безработных в возрасте 60 и более лет по регионам РФ
* num_emagegroup_to20 - Численность занятых в экономике регионов РФ в возрасте до 20 лет
* num_emagegroup_20-29 - Численность занятых в экономике регионов РФ в возрасте от 20 до 29 лет
* num_emagegroup_30-39 - Численность занятых в экономике регионов РФ в возрасте от 30 до 39 лет
* num_emagegroup_40-49 - Численность занятых в экономике регионов РФ в возрасте от 40 до 49 лет
* num_emagegroup_50-59 - Численность занятых в экономике регионов РФ в возрасте от 50 до 59 лет
* num_emagegroup_60older - Численность занятых в экономике регионов РФ в возрасте 60 и более лет
* year - Отчетный год

### Импорт библиотек
"""

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from scipy.stats import randint, uniform
# %matplotlib inline
sns.set(style="ticks")
# скроем предупреждения о возможных ошибках для лучшей читаемости
import warnings
warnings.filterwarnings('ignore')

"""### Загрузка данных

"""

data = pd.read_csv('data.csv')

"""### Анализ датасета
##### Основные характеристики датасета
Первые 5 строк датасета:
"""

data.head()

"""Размер датасета:"""

data.shape

"""Столбцы:"""

data.columns

"""Удалим лишние столбцы:"""

data = data.filter(['num_economactivepopulation_all',
                    'employed_num_all', 'unemployed_num_all',
                    'eactivity_lvl', 'employment_lvl','unemployment_lvl',
                    'num_unagegroup_to20', 'num_unagegroup_20-29',
                    'num_unagegroup_30-39', 'num_unagegroup_40-49',
                    'num_unagegroup_50-59',  'num_unagegroup_60older',
                    'year'])
data.head()

"""### Предварительная обработка данных

#### Заменим названия колонок
"""

data.rename(columns = {
    'num_economactivepopulation_all' : 'Численность населения',
    'employed_num_all' : 'Занятые в экономике',
    'unemployed_num_all' : 'Безработные',
    'eactivity_lvl' : 'Уровень экономической активности',
    'employment_lvl' : 'Уровень занятости',
    'unemployment_lvl' : 'Уровень безработицы',
    'num_unagegroup_to20' : 'Численность безработных (до 20 лет)',
    'num_unagegroup_20-29' : 'Численность безработных (от 20 до 29 лет)',
    'num_unagegroup_30-39' : 'Численность безработных (от 30 до 39 лет)',
    'num_unagegroup_40-49' : 'Численность безработных (от 40 до 49 лет)',
    'num_unagegroup_50-59' : 'Численность безработных (от 50 до 59 лет)',
    'num_unagegroup_60older' : 'Численность безработных (60 и более лет)',
    'year' : 'Год',
}, inplace = True)

list(data.columns)

"""#### Проверка и обработка пропущенных значений
Определим столбцы с пропусками данных:
"""

data.isnull().sum()

"""Видим, что в каждом стобце есть пропущенные значения, кроме "Год"."""

# проверяем пропуска, во всех ли столбцах они есть
data[pd.isnull(data).any(axis=1)]

"""Видим, что строка 40 и 131 с пустыми значениями. Предположительно по этим строкам не фиксировались данные, поэтому мы исключим их."""

# удаляем строки, в которых отсутствуют данные по всем ячейкам. Это строки 40 и 131
data.drop(labels = [40,131],axis = 0, inplace = True)

"""Проверим сколько пропусков у нас осталось сейчас."""

data.isnull().sum()

# Замена пропущенных значений средними значением по столбцу
data['Численность безработных (до 20 лет)'].fillna(data['Численность безработных (до 20 лет)'].mean(), inplace=True)
data['Численность безработных (60 и более лет)'].fillna(data['Численность безработных (60 и более лет)'].mean(), inplace=True)

data.isnull().sum()

"""#### Проверка и обработка дубликатов"""

# проверим количество дубликатов (метод duplicated())
sum_duplicated = data.duplicated().sum()
print('Количество дубликатов: ', sum_duplicated)

"""#### Проверка и обработка типов данных"""

data.info()

# изменением на int
data['Численность населения'] = data['Численность населения'].astype(int)
data['Занятые в экономике'] = data['Занятые в экономике'].astype(int)
data['Безработные'] = data['Безработные'].astype(int)
# изменением на object
data['Год'] = data['Год'].astype(object) # Целевой признак

data.dtypes

"""### Кодирование категориальных признаков

Возьмем в качестве целевой переменной стоблец "Год"
"""

data['Год'].unique()

"""Для решения задачи классификации выберем два класса. Это год 2001 и 2019"""

data[data['Год'].isin([2001, 2019])]

"""Создадим новый датасет, в котором хранятся данные за 2001 и 2019 год"""

data_1 = data[data['Год'].isin([2001, 2019])]

data_1['Год'].unique()

"""Убедимся, что целевой признак для задачи бинарной классификации содержит только 0 и 1"""

# присваиваем целочисленные значения для каждой категории
labelencoder = LabelEncoder()
data_1['Год'] = labelencoder.fit_transform(data_1['Год'])
data_1 = data_1.astype({"Год":"int64"})
data_1['Год'].unique()

# Оценим дисбаланс классов
fig, ax = plt.subplots(figsize=(2,2))
plt.hist(data_1['Год'])
plt.show()

data_1['Год'].value_counts()

# посчитаем дисбаланс классов
total = data_1.shape[0]
class_0, class_1 = data_1['Год'].value_counts()
print('Класс 0 составляет {}%, а класс 1 составляет {}%.'
      .format(round(class_0 / total, 4)*100, round(class_1 / total, 4)*100))

"""Присутствует незначительный дисбаланс классов.

## Скачаем новый датасет "data_1"
"""

from IPython.display import HTML, display
import base64
import requests

def check_internet_connection(url='http://www.google.com/', timeout=5):
    try:
        _ = requests.get(url, timeout=timeout)
        return True
    except requests.ConnectionError:
        return False

def create_download_link(data, title="Скачать CSV файл", filename="data_1.csv"):
  csv = data_1.to_csv(index=False)
  b64 = base64.b64encode(csv.encode()).decode()
  href = f'<a href="data:file/csv;base64,{b64}" download="{filename}">{title}</a>'
  return HTML(href)

if check_internet_connection():
    download_link = create_download_link(data)
    display(download_link)

"""### Масштабирование данных"""

data_1.dtypes

"""Категориальный признак "Год" был закодировани заранее, другие категориальные признаки отсутствуют."""

# Числовые колонки для масштабирования
scale_cols = [ 'Численность населения',
               'Занятые в экономике',
               'Безработные',
               'Уровень экономической активности',
               'Уровень занятости',
               'Уровень безработицы',
                'Численность безработных (до 20 лет)',
               'Численность безработных (от 20 до 29 лет)',
               'Численность безработных (от 30 до 39 лет)',
               'Численность безработных (от 40 до 49 лет)',
               'Численность безработных (от 50 до 59 лет)',
               'Численность безработных (60 и более лет)']

# Преобразование значения признаков таким образом,
# чтобы они находились в диапазоне от 0 до 1
sc1 = MinMaxScaler()
sc1_data = sc1.fit_transform(data_1[scale_cols])

# Добавим масштабированные данные в набор данных
for i in range(len(scale_cols)):
    col = scale_cols[i]
    new_col_name = col + '_scaled'
    data_1[new_col_name] = sc1_data[:,i]

data_1.head()

# Проверим, что масштабирование не повлияло на распределение данных
for col in scale_cols:
    col_scaled = col + '_scaled'

    fig, ax = plt.subplots(1, 2, figsize=(8,3))
    ax[0].hist(data_1[col], 50)
    ax[1].hist(data_1[col_scaled], 50)
    ax[0].title.set_text(col)
    ax[1].title.set_text(col_scaled)
    plt.show()

"""Масштабирование данных не повлияло на распределение данных

### Корреляционный анализ данных
"""

# Воспользуемся наличием тестовых выборок,
# включив их в корреляционную матрицу
corr_cols_1 = scale_cols + ['Год']
corr_cols_1

scale_cols_postfix = [x+'_scaled' for x in scale_cols]
corr_cols_2 = scale_cols_postfix + ['Год']
corr_cols_2

fig, ax = plt.subplots(figsize=(10,5))
sns.heatmap(data_1[corr_cols_1].corr(), annot=True, fmt='.2f')
ax.set_title('Исходные данные (до масштабирования)')
plt.show()

fig, ax = plt.subplots(figsize=(10,5))
sns.heatmap(data_1[corr_cols_2].corr(), annot=True, fmt='.2f')
ax.set_title('Масштабированные данные')
plt.show()

"""На основе корреляционной матрицы можно сделать следующие выводы:

Корреляционные матрицы для исходных и масштабированных данных совпадают.

Целевой признак классификации "Год" наиболее сильно коррелирует с "Уровнем занятости" (0.07), "Занятые в экономике" (0.01), "Численность населения" (0.00). Эти признаки обязательно следует оставить в модели классификации.

### Формирование обучающей и тестовой выборок на основе исходного набора данных
"""

X = data_1[['Уровень занятости', 'Занятые в экономике', 'Численность населения']] # Наименование признаков
y = data_1['Год'] # Целевая переменная

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)

# Размер обучающей выборки
X_train.shape, y_train.shape

# Размер тестовой выборки
X_test.shape, y_test.shape

"""### Выбор алгоритмов машинного обучения для решения задачи классификации

* Логистическая регрессия

Используется для оценки дискретных значений, обычно двоичных, таких как 0 и 1, да или нет. Он предсказывает вероятность принадлежности экземпляра к классу, что делает его незаменимым для задач бинарной классификации, таких как обнаружение спама или диагностика заболеваний.


* К-ближайших соседей (KNN)

KNN классифицирует объект на основе меток классов его ближайщих соседей в пространстве признаков.

* Случайный лес


 Он создает множество решающих деревьев, то есть независимых моделей, и использует их для предсказания классов объектов.

### Метрики для оценки качества моделей

В качестве метрик для решения задачи классификации будем использовать:

* Accuary (точность):

Доля правильно классифицированных объектов среди всех объектов.

* Precision (Точность):

Доля истинно положительных среди всех объектов, которые модель классифицировала как положительные. Она измеряет, насколько много из предсказанных положительных случаев действительно положительные.

* Recall (Полнота):

Доля истинно положительных объектов среди всех действительно положительных объектов. Она измеряет, насколько много реальных положительных случаев было предсказано моделью.

* Метрика F1-мера:

Гармоническое среднее между точностью и полнотой.

### Логическая регрессия
"""

# Создание и обучение модели логистической регрессии
model = LogisticRegression()
model.fit (X_train, y_train)
y_pred = model.predict(X_test)

# Предсказание и оценка производительности
accuracy_model = accuracy_score(y_test, y_pred)
precision_model = precision_score(y_test, y_pred)
recall_model = recall_score(y_test, y_pred)
f1_model = f1_score(y_test, y_pred)

"""### К-ближайших соседей"""

# Создание и обучение модели К-ближайших соседей
model2 = KNeighborsClassifier()
model2.fit (X_train, y_train)
y_pred2 = model2.predict(X_test)

# Предсказание и оценка производительности
accuracy_model2 = accuracy_score(y_test, y_pred2)
precision_model2 = precision_score(y_test, y_pred2)
recall_model2 = recall_score(y_test, y_pred2)
f1_model2 = f1_score(y_test, y_pred2)

"""### Случайный лес"""

# Создание и обучение модели случайный лес
model3 = RandomForestClassifier()
model3.fit(X_train, y_train)
y_pred3 = model3.predict(X_test)

# Предсказание и оценка производительности
accuracy_model3 = accuracy_score(y_test, y_pred3)
precision_model3 = precision_score(y_test, y_pred3)
recall_model3 = recall_score(y_test, y_pred3)
f1_model3 = f1_score(y_test, y_pred3)

"""### Результаты сравнения моделей"""

print("Logistic Regression:")
print(f"Accuracy: {accuracy_model}")
print(f"Precision: {precision_model}")
print(f"Recall: {recall_model}")
print(f"F1 Score: {f1_model}")
print("\nKNN:")
print(f"Accuracy: {accuracy_model2}")
print(f"Precision: {precision_model2}")
print(f"Recall: {recall_model2}")
print(f"F1 Score: {f1_model2}")
print("\nRandom Forest:")
print(f"Accuracy: {accuracy_model3}")
print(f"Precision: {precision_model3}")
print(f"Recall: {recall_model3}")
print(f"F1 Score: {f1_model3}")

# Результаты моделей в виде графиков
results = {
    'Logistic Regression': {'Accuracy': 0.8108, 'Precision': 0.8261, 'Recall': 0.8636, 'F1 Score': 0.8444},
    'KNN': {'Accuracy': 0.5405, 'Precision': 0.6471, 'Recall': 0.5, 'F1 Score': 0.5641},
    'Random Forest': {'Accuracy': 0.3783, 'Precision': 0.4667, 'Recall': 0.3181, 'F1 Score': 0.3783}
}

# Построение графиков для каждой метрики
metrics = list(results['Logistic Regression'].keys())
for metric in metrics:
    values = [results[model][metric] for model in results]
    plt.figure(figsize=(8, 6))
    plt.bar(results.keys(), values, color=['blue', 'green', 'red'])
    plt.title(f'{metric} Сравнение')
    plt.xlabel('Model')
    plt.ylabel(metric)
    plt.ylim(0, 1)  # Установка границ для оси y
    plt.show()

"""### Вывод результата анализа

1. Логистическая регрессия показала наилучший результат среди всех моделей. Она имеет наивысшие показатели по всем метрикам: точность (0.8108), точность (0.8261), полнота (0.8636) и F1-мера (0.8444). Это указывает на хорошее общее качество модели, уравновешенное между правильными предсказаниями и пропущенными положительными случаями.


2. К-ближайших соседей (KNN) имеет средние результаты по сравнению с другими моделями. Точность (0.5405), точность (0.6471), полнота (0.5) и F1-мера (0.5641) значительно ниже, чем у логистической регрессии. Это указывает на то, что KNN менее эффективен для данной задачи.


3. Случайный лес показала наихудшие результаты. Точность (0.3783), точность (0.4667), полнота (0.3181) и F1-мера (0.3783) являются самыми низкими среди всех моделей. Это может указывать на проблемы с переобучением или недостаточной настройкой гипепараметров модели.

На основе проведенного сравнения, логистическая регрессия является наилучшей моделью для данной задачи классификации. Она демонстрирует высокие показатели по всем ключевым метрикам, что делает ее предпочительным выбором.


"""

